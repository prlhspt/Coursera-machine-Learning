{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "human .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPWl38XgqB09tJFNbPQBaB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prlhspt/Coursera-machine-Learning/blob/master/digit_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGr0-GyV9f4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YwbZVlq9pem",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4b061b7a-5321-4936-c324-dda771f0ed02"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Y_encoded = tf.keras.utils.to_categorical(Y)\n",
        "Y_encoded[0], Y_encoded[50], Y_encoded[100]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
              " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
              " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.], dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT8FHTML97gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 2020\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "X = digits.data\n",
        "Y = digits.target"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDF44fVQ95PT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "e2a648c2-acf0-44b4-e3c2-fb4686db9a47"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y_encoded, test_size=0.2, random_state=2020)\n",
        "# 모델 설정\n",
        "model = Sequential([\n",
        "    Dense(128, input_dim=64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax'),\n",
        "]) \n",
        "model.summary()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_28\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_133 (Dense)            (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dropout_72 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (None, 16)                2064      \n",
            "_________________________________________________________________\n",
            "dropout_73 (Dropout)         (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 10,554\n",
            "Trainable params: 10,554\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFhLwbz5-I3A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d3ac3997-d0fe-4534-fb7c-234008e2255d"
      },
      "source": [
        "# 모델 저장 폴더 설정\n",
        "import os\n",
        "MODEL_DIR = './model_test/'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.mkdir(MODEL_DIR)\n",
        "# 모델 저장 조건 설정\n",
        "modelpath = MODEL_DIR + \"final{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
        "checkpointer_callback = ModelCheckpoint(filepath=modelpath, monitor='val_loss', \n",
        "                               verbose=1, save_best_only=True)\n",
        "# 모델 컴파일 \n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 자동 중단 설정\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=500)\n",
        "\n",
        "# 모델 실행 및 저장\n",
        "history = model.fit(X_train, y_train, validation_split=0.2, epochs=3500, batch_size=100\n",
        "                    ,verbose=0, callbacks=[early_stopping_callback, checkpointer_callback])"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.24316, saving model to ./model_test/final001-2.2432.hdf5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 2.24316\n",
            "\n",
            "Epoch 00013: val_loss improved from 2.24316 to 2.21538, saving model to ./model_test/final013-2.2154.hdf5\n",
            "\n",
            "Epoch 00014: val_loss improved from 2.21538 to 2.17039, saving model to ./model_test/final014-2.1704.hdf5\n",
            "\n",
            "Epoch 00015: val_loss improved from 2.17039 to 2.13118, saving model to ./model_test/final015-2.1312.hdf5\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.13118 to 2.06792, saving model to ./model_test/final016-2.0679.hdf5\n",
            "\n",
            "Epoch 00017: val_loss improved from 2.06792 to 1.97304, saving model to ./model_test/final017-1.9730.hdf5\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.97304 to 1.85976, saving model to ./model_test/final018-1.8598.hdf5\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.85976 to 1.82497, saving model to ./model_test/final019-1.8250.hdf5\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.82497 to 1.80008, saving model to ./model_test/final020-1.8001.hdf5\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.80008 to 1.78511, saving model to ./model_test/final021-1.7851.hdf5\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.78511 to 1.72377, saving model to ./model_test/final022-1.7238.hdf5\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.72377 to 1.68290, saving model to ./model_test/final023-1.6829.hdf5\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.68290 to 1.62767, saving model to ./model_test/final024-1.6277.hdf5\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.62767 to 1.58110, saving model to ./model_test/final025-1.5811.hdf5\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.58110 to 1.55590, saving model to ./model_test/final026-1.5559.hdf5\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.55590 to 1.53792, saving model to ./model_test/final027-1.5379.hdf5\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.53792 to 1.46766, saving model to ./model_test/final028-1.4677.hdf5\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.46766 to 1.42966, saving model to ./model_test/final029-1.4297.hdf5\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.42966 to 1.36654, saving model to ./model_test/final030-1.3665.hdf5\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.36654 to 1.33662, saving model to ./model_test/final031-1.3366.hdf5\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.33662 to 1.27640, saving model to ./model_test/final032-1.2764.hdf5\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.27640 to 1.22847, saving model to ./model_test/final033-1.2285.hdf5\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.22847 to 1.18843, saving model to ./model_test/final034-1.1884.hdf5\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.18843 to 1.12946, saving model to ./model_test/final035-1.1295.hdf5\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.12946 to 1.08514, saving model to ./model_test/final036-1.0851.hdf5\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.08514 to 1.07083, saving model to ./model_test/final037-1.0708.hdf5\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.07083 to 1.01680, saving model to ./model_test/final038-1.0168.hdf5\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.01680 to 0.95539, saving model to ./model_test/final039-0.9554.hdf5\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.95539 to 0.85620, saving model to ./model_test/final040-0.8562.hdf5\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.85620 to 0.80122, saving model to ./model_test/final041-0.8012.hdf5\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.80122 to 0.74417, saving model to ./model_test/final042-0.7442.hdf5\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.74417 to 0.68463, saving model to ./model_test/final043-0.6846.hdf5\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.68463 to 0.67610, saving model to ./model_test/final044-0.6761.hdf5\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.67610 to 0.65639, saving model to ./model_test/final045-0.6564.hdf5\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.65639 to 0.63233, saving model to ./model_test/final046-0.6323.hdf5\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.63233 to 0.60862, saving model to ./model_test/final047-0.6086.hdf5\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.60862 to 0.55373, saving model to ./model_test/final048-0.5537.hdf5\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.55373 to 0.52723, saving model to ./model_test/final049-0.5272.hdf5\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.52723\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.52723 to 0.51575, saving model to ./model_test/final051-0.5157.hdf5\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.51575 to 0.47294, saving model to ./model_test/final052-0.4729.hdf5\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.47294\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.47294 to 0.44843, saving model to ./model_test/final054-0.4484.hdf5\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.44843 to 0.42551, saving model to ./model_test/final055-0.4255.hdf5\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.42551 to 0.40155, saving model to ./model_test/final056-0.4016.hdf5\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.40155\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.40155 to 0.37471, saving model to ./model_test/final058-0.3747.hdf5\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.37471 to 0.36075, saving model to ./model_test/final059-0.3608.hdf5\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.36075\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.36075 to 0.35630, saving model to ./model_test/final061-0.3563.hdf5\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.35630 to 0.32735, saving model to ./model_test/final062-0.3273.hdf5\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.32735 to 0.30893, saving model to ./model_test/final063-0.3089.hdf5\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.30893 to 0.30512, saving model to ./model_test/final064-0.3051.hdf5\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.30512 to 0.29879, saving model to ./model_test/final065-0.2988.hdf5\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.29879 to 0.29757, saving model to ./model_test/final066-0.2976.hdf5\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.29757 to 0.28015, saving model to ./model_test/final067-0.2802.hdf5\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.28015 to 0.26231, saving model to ./model_test/final068-0.2623.hdf5\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.26231 to 0.25205, saving model to ./model_test/final069-0.2521.hdf5\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.25205\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.25205\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.25205\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.25205\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.25205 to 0.23882, saving model to ./model_test/final074-0.2388.hdf5\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.23882\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.23882 to 0.23844, saving model to ./model_test/final076-0.2384.hdf5\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.23844 to 0.23828, saving model to ./model_test/final077-0.2383.hdf5\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.23828\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.23828\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.23828 to 0.22215, saving model to ./model_test/final080-0.2222.hdf5\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.22215\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.22215\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.22215\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.22215 to 0.20827, saving model to ./model_test/final084-0.2083.hdf5\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.20827 to 0.20777, saving model to ./model_test/final085-0.2078.hdf5\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.20777 to 0.19891, saving model to ./model_test/final086-0.1989.hdf5\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.19891 to 0.19636, saving model to ./model_test/final087-0.1964.hdf5\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.19636 to 0.19602, saving model to ./model_test/final088-0.1960.hdf5\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.19602 to 0.18728, saving model to ./model_test/final089-0.1873.hdf5\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.18728 to 0.18505, saving model to ./model_test/final090-0.1850.hdf5\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.18505\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.18505 to 0.18306, saving model to ./model_test/final092-0.1831.hdf5\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.18306 to 0.18085, saving model to ./model_test/final093-0.1808.hdf5\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.18085\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.18085 to 0.17477, saving model to ./model_test/final095-0.1748.hdf5\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.17477\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.17477\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.17477\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.17477\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.17477\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.17477 to 0.17477, saving model to ./model_test/final101-0.1748.hdf5\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.17477\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.17477 to 0.16746, saving model to ./model_test/final103-0.1675.hdf5\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.16746 to 0.15566, saving model to ./model_test/final104-0.1557.hdf5\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.15566\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.15566\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.15566 to 0.14055, saving model to ./model_test/final107-0.1405.hdf5\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.14055\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.14055\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.14055\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.14055\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.14055\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.14055\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.14055\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.14055\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.14055 to 0.13947, saving model to ./model_test/final116-0.1395.hdf5\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.13947 to 0.13496, saving model to ./model_test/final117-0.1350.hdf5\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.13496 to 0.12337, saving model to ./model_test/final118-0.1234.hdf5\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.12337 to 0.11873, saving model to ./model_test/final119-0.1187.hdf5\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.11873\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.11873\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.11873\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.11873\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.11873\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.11873\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.11873\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.11873\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.11873\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.11873 to 0.11601, saving model to ./model_test/final129-0.1160.hdf5\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.11601\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.11601\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.11601\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.11601 to 0.11198, saving model to ./model_test/final133-0.1120.hdf5\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.11198 to 0.10886, saving model to ./model_test/final134-0.1089.hdf5\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.10886\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.10886\n",
            "\n",
            "Epoch 00137: val_loss improved from 0.10886 to 0.10167, saving model to ./model_test/final137-0.1017.hdf5\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.10167\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.10167\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.10167\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.10167\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.10167 to 0.09485, saving model to ./model_test/final142-0.0948.hdf5\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.09485 to 0.09137, saving model to ./model_test/final150-0.0914.hdf5\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.09137\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.09137\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.09137\n",
            "\n",
            "Epoch 00154: val_loss improved from 0.09137 to 0.07940, saving model to ./model_test/final154-0.0794.hdf5\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.07940\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.07940 to 0.07232, saving model to ./model_test/final171-0.0723.hdf5\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.07232\n",
            "\n",
            "Epoch 00201: val_loss improved from 0.07232 to 0.07012, saving model to ./model_test/final201-0.0701.hdf5\n",
            "\n",
            "Epoch 00202: val_loss improved from 0.07012 to 0.06257, saving model to ./model_test/final202-0.0626.hdf5\n",
            "\n",
            "Epoch 00203: val_loss improved from 0.06257 to 0.05807, saving model to ./model_test/final203-0.0581.hdf5\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.05807\n",
            "\n",
            "Epoch 00247: val_loss improved from 0.05807 to 0.05738, saving model to ./model_test/final247-0.0574.hdf5\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.05738\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.05738\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.05738\n",
            "\n",
            "Epoch 00251: val_loss improved from 0.05738 to 0.05374, saving model to ./model_test/final251-0.0537.hdf5\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.05374\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.05374\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.05374\n",
            "\n",
            "Epoch 00255: val_loss improved from 0.05374 to 0.04321, saving model to ./model_test/final255-0.0432.hdf5\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.04321\n",
            "\n",
            "Epoch 00402: val_loss improved from 0.04321 to 0.03899, saving model to ./model_test/final402-0.0390.hdf5\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.03899\n",
            "\n",
            "Epoch 00488: val_loss improved from 0.03899 to 0.03462, saving model to ./model_test/final488-0.0346.hdf5\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00595: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00633: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00682: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 0.03462\n",
            "\n",
            "Epoch 00708: val_loss improved from 0.03462 to 0.03257, saving model to ./model_test/final708-0.0326.hdf5\n",
            "\n",
            "Epoch 00709: val_loss improved from 0.03257 to 0.03251, saving model to ./model_test/final709-0.0325.hdf5\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00721: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00734: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00740: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00745: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00751: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00752: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00753: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00754: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00755: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00756: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00757: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00758: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00759: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00760: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00761: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00762: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00763: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00764: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00765: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00766: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00767: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00768: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00769: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00770: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00771: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00772: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00773: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00774: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00775: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00776: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00777: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00778: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00779: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00780: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00781: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00782: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00783: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00784: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00785: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00786: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00787: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00788: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00789: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00790: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00791: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00792: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00793: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00794: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00795: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00796: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00797: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00798: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00799: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00800: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00801: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00802: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00803: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00804: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00805: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00806: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00807: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00808: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00809: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00810: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00811: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00812: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00813: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00814: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00815: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00816: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00817: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00818: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00819: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00820: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00821: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00822: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00823: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00824: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00825: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00826: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00827: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00828: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00829: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00830: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00831: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00832: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00833: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00834: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00835: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00836: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00837: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00838: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00839: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00840: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00841: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00842: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00843: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00844: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00845: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00846: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00847: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00848: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00849: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00850: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00851: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00852: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00853: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00854: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00855: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00856: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00857: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00858: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00859: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00860: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00861: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00862: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00863: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00864: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00865: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00866: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00867: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00868: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00869: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00870: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00871: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00872: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00873: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00874: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00875: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00876: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00877: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00878: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00879: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00880: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00881: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00882: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00883: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00884: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00885: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00886: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00887: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00888: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00889: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00890: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00891: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00892: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00893: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00894: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00895: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00896: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00897: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00898: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00899: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00900: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00901: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00902: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00903: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00904: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00905: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00906: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00907: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00908: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00909: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00910: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00911: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00912: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00913: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00914: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00915: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00916: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00917: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00918: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00919: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00920: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00921: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00922: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00923: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00924: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00925: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00926: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00927: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00928: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00929: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00930: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00931: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00932: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00933: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00934: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00935: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00936: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00937: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00938: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00939: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00940: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00941: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00942: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00943: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00944: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00945: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00946: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00947: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00948: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00949: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00950: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00951: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00952: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00953: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00954: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00955: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00956: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00957: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00958: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00959: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00960: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00961: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00962: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00963: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00964: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00965: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00966: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00967: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00968: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00969: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00970: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00971: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00972: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00973: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00974: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00975: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00976: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00977: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00978: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00979: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00980: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00981: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00982: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00983: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00984: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00985: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00986: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00987: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00988: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00989: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00990: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00991: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00992: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00993: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00994: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00995: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00996: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00997: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00998: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 00999: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01000: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01001: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01002: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01003: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01004: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01005: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01006: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01007: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01008: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01009: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01010: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01011: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01012: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01013: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01014: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01015: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01016: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01017: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01018: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01019: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01020: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01021: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01022: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01023: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01024: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01025: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01026: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01027: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01028: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01029: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01030: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01031: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01032: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01033: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01034: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01035: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01036: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01037: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01038: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01039: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01040: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01041: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01042: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01043: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01044: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01045: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01046: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01047: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01048: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01049: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01050: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01051: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01052: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01053: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01054: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01055: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01056: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01057: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01058: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01059: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01060: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01061: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01062: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01063: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01064: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01065: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01066: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01067: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01068: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01069: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01070: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01071: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01072: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01073: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01074: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01075: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01076: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01077: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01078: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01079: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01080: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01081: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01082: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01083: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01084: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01085: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01086: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01087: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01088: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01089: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01090: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01091: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01092: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01093: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01094: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01095: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01096: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01097: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01098: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01099: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01100: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01101: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01102: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01103: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01104: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01105: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01106: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01107: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01108: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01109: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01110: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01111: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01112: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01113: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01114: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01115: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01116: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01117: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01118: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01119: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01120: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01121: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01122: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01123: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01124: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01125: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01126: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01127: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01128: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01129: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01130: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01131: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01132: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01133: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01134: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01135: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01136: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01137: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01138: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01139: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01140: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01141: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01142: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01143: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01144: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01145: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01146: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01147: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01148: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01149: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01150: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01151: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01152: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01153: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01154: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01155: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01156: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01157: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01158: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01159: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01160: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01161: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01162: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01163: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01164: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01165: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01166: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01167: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01168: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01169: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01170: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01171: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01172: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01173: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01174: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01175: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01176: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01177: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01178: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01179: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01180: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01181: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01182: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01183: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01184: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01185: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01186: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01187: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01188: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01189: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01190: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01191: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01192: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01193: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01194: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01195: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01196: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01197: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01198: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01199: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01200: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01201: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01202: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01203: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01204: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01205: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01206: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01207: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01208: val_loss did not improve from 0.03251\n",
            "\n",
            "Epoch 01209: val_loss did not improve from 0.03251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdRA-g-C_ML_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1b7b6f3a-d237-44c6-91fc-53872fe82497"
      },
      "source": [
        "# 결과 출력 \n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_test, y_test, verbose=2)[1]))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/12 - 0s - loss: 0.1439 - accuracy: 0.9833\n",
            "\n",
            " Accuracy: 0.9833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6Al6aJN-0Rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tensorflow.keras.models import load_model\n",
        "model = load_model('./model_test/final758-0.1650.hdf5')"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaajUaV9_TST",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "fd3ec642-da78-4437-f9d6-284e177bffc6"
      },
      "source": [
        "# 결과 출력 \n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X_test, y_test, verbose=2)[1]))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/12 - 0s - loss: 0.1896 - accuracy: 0.9667\n",
            "\n",
            " Accuracy: 0.9667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTGKvHZoAWMa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "outputId": "d78c26cf-78ab-46fd-f8da-73131d7dc219"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# y_vloss에 테스트셋으로 실험 결과의 오차 값을 저장\n",
        "\n",
        "y_vloss=history.history['val_loss']\n",
        "\n",
        "# y_acc에 학습셋으로 측정한 정확도의 값을 저장\n",
        "\n",
        "y_acc=history.history['accuracy']\n",
        "\n",
        "# x 값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시\n",
        "\n",
        "x_len = np.arange(len(y_acc))\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=2)\n",
        "\n",
        "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=2)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}